{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed using the checking column: 215\n",
      "\n",
      "Counts of each label:\n",
      "polarizing     297\n",
      "populist        93\n",
      "extremist        3\n",
      "neutral       1715\n",
      "dtype: int64\n",
      "\n",
      "Co-occurrence of labels:\n",
      "            polarizing  populist  extremist  neutral\n",
      "polarizing           0        66          2        1\n",
      "populist            66         0          0        1\n",
      "extremist            2         0          0        0\n",
      "neutral              1         1          0        0\n",
      "\n",
      "Number of rows where 'neutral' is not exclusive with other labels: 2\n",
      "\n",
      "Number of rows with quotation marks: 138\n",
      "\n",
      "Number of duplicate entries in validation data: 77\n",
      "\n",
      "Final training data shape: (1318, 9)\n",
      "Final validation data shape: (684, 9)\n",
      "\n",
      "Total rows in initial data: 2269\n",
      "Total rows after processing: 2002\n",
      "Total rows removed during processing: 267\n",
      "\n",
      "Breakdown of rows removed:\n",
      " - Removed by checking column: 215\n",
      " - Removed due to 'neutral' conflicts: 2\n",
      " - Removed by max sentences filter: 0\n",
      " - Removed by max characters filter: 0\n",
      " - Removed duplicates in validation data: 77\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = 'labeled_speech_data'\n",
    "xlsx_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
    "label_cols = ['polarizing', 'populist', 'extremist', 'neutral']\n",
    "\n",
    "dfs = []\n",
    "\n",
    "total_rows_initial = 0\n",
    "rows_removed_checking_col = 0\n",
    "rows_removed_neutral_conflict = 0\n",
    "rows_removed_max_sentences = 0\n",
    "rows_removed_max_characters = 0\n",
    "\n",
    "# Process each file\n",
    "for file in xlsx_files:\n",
    "    # Ignore columns beyond the 7th column\n",
    "    df = pd.read_excel(file, usecols=range(7), header=0)\n",
    "    total_rows_initial += df.shape[0]\n",
    "    \n",
    "    # Assign standard column names\n",
    "    df.columns = ['speech_content', 'speech_id_long', 'polarizing', 'populist', 'extremist', 'neutral', 'check_col']\n",
    "    \n",
    "    # Add 'is_validation' flag\n",
    "    df['is_validation'] = 'validation' in file.lower()\n",
    "    \n",
    "    # Keep track of the source file (optional)\n",
    "    df['source_file'] = os.path.basename(file)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Remove rows based on the 'check_col' column (1 or x indicating keep row)\n",
    "combined_df['check_col'] = combined_df['check_col'].astype(str).str.lower()\n",
    "rows_before_check = combined_df.shape[0]\n",
    "combined_df = combined_df[combined_df['check_col'].isin(['1', 'x'])]\n",
    "rows_after_check = combined_df.shape[0]\n",
    "rows_removed_checking_col = rows_before_check - rows_after_check\n",
    "\n",
    "\n",
    "print(\"Number of rows removed using the checking column:\", rows_removed_checking_col)\n",
    "\n",
    "# Compute statistics on labels\n",
    "print(\"\\nCounts of each label:\")\n",
    "label_counts = combined_df[label_cols].sum()\n",
    "print(label_counts)\n",
    "\n",
    "# Co-occurrence of labels\n",
    "co_occurrence = pd.DataFrame(0, index=label_cols, columns=label_cols)\n",
    "\n",
    "for label1, label2 in combinations(label_cols, 2):\n",
    "    count = ((combined_df[label1] == 1) & (combined_df[label2] == 1)).sum()\n",
    "    co_occurrence.loc[label1, label2] = count\n",
    "    co_occurrence.loc[label2, label1] = count\n",
    "\n",
    "print(\"\\nCo-occurrence of labels:\")\n",
    "print(co_occurrence)\n",
    "\n",
    "# Find and remove rows where 'neutral' is not exclusive\n",
    "neutral_conflicts = combined_df[\n",
    "    (combined_df['neutral'] == 1) & (combined_df[label_cols[:-1]].sum(axis=1) > 0)\n",
    "]\n",
    "\n",
    "rows_removed_neutral_conflict = neutral_conflicts.shape[0]\n",
    "print(\"\\nNumber of rows where 'neutral' is not exclusive with other labels:\", rows_removed_neutral_conflict)\n",
    "\n",
    "# Remove conflicting rows\n",
    "combined_df = combined_df.drop(neutral_conflicts.index)\n",
    "\n",
    "# Clean the text content in 'speech_content' column\n",
    "def clean_text(text):\n",
    "    # Remove leading/trailing whitespaces and linebreaks\n",
    "    text = text.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "combined_df['speech_content'] = combined_df['speech_content'].astype(str).apply(clean_text)\n",
    "\n",
    "# Count how many speech excerpts contain quotation marks\n",
    "quotation_marks = ['\"', \"'\", '“', '”', '‘', '’', '«', '»', '‹', '›']\n",
    "quotation_pattern = '[' + ''.join(quotation_marks) + ']'\n",
    "num_quotes = combined_df['speech_content'].str.contains(quotation_pattern).sum()\n",
    "print(\"\\nNumber of rows with quotation marks:\", num_quotes)\n",
    "\n",
    "# Filter rows based on max sentences and max characters\n",
    "def count_sentences(text):\n",
    "    # Count number of sentence-ending punctuation marks\n",
    "    return len(re.findall(r'[.!?:]', text))\n",
    "\n",
    "# Specify max sentences and max characters (adjust as needed)\n",
    "max_sentences = None  \n",
    "max_characters = None  \n",
    "\n",
    "# Filter rows based on max sentences\n",
    "if max_sentences is not None:\n",
    "    before_sentences = combined_df.shape[0]\n",
    "    combined_df = combined_df[combined_df['speech_content'].apply(count_sentences) <= max_sentences]\n",
    "    after_sentences = combined_df.shape[0]\n",
    "    rows_removed_max_sentences = before_sentences - after_sentences\n",
    "    print(\"\\nRows removed based on max sentences:\", rows_removed_max_sentences)\n",
    "\n",
    "# Filter rows based on max characters\n",
    "if max_characters is not None:\n",
    "    before_chars = combined_df.shape[0]\n",
    "    combined_df = combined_df[combined_df['speech_content'].str.len() <= max_characters]\n",
    "    after_chars = combined_df.shape[0]\n",
    "    rows_removed_max_characters = before_chars - after_chars\n",
    "    print(\"Rows removed based on max characters:\", rows_removed_max_characters)\n",
    "\n",
    "# Separate validation and training data\n",
    "validation_df = combined_df[combined_df['is_validation'] == True]\n",
    "training_df = combined_df[combined_df['is_validation'] == False]\n",
    "\n",
    "# Check for duplicates in validation data based on 'speech_id_long'\n",
    "duplicates = validation_df[validation_df.duplicated(subset=['speech_id_long'], keep=False)]\n",
    "\n",
    "num_duplicates = duplicates.shape[0]\n",
    "print(\"\\nNumber of duplicate entries in validation data:\", num_duplicates)\n",
    "\n",
    "# Remove duplicates\n",
    "validation_df = validation_df.drop_duplicates(subset=['speech_id_long'])\n",
    "training_df = training_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)\n",
    "print(\"\\nFinal training data shape:\", training_df.shape)\n",
    "print(\"Final validation data shape:\", validation_df.shape)\n",
    "\n",
    "# Additional statistics\n",
    "total_rows_final = training_df.shape[0] + validation_df.shape[0]\n",
    "total_rows_removed = total_rows_initial - total_rows_final\n",
    "\n",
    "print(\"\\nTotal rows in initial data:\", total_rows_initial)\n",
    "print(\"Total rows after processing:\", total_rows_final)\n",
    "print(\"Total rows removed during processing:\", total_rows_removed)\n",
    "\n",
    "print(\"\\nBreakdown of rows removed:\")\n",
    "print(\" - Removed by checking column:\", rows_removed_checking_col)\n",
    "print(\" - Removed due to 'neutral' conflicts:\", rows_removed_neutral_conflict)\n",
    "print(\" - Removed by max sentences filter:\", rows_removed_max_sentences)\n",
    "print(\" - Removed by max characters filter:\", rows_removed_max_characters)\n",
    "print(\" - Removed duplicates in validation data:\", num_duplicates)\n",
    "\n",
    "# Optionally, save the final datasets\n",
    "# training_df.to_excel('training_data.xlsx', index=False)\n",
    "# validation_df.to_excel('validation_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
